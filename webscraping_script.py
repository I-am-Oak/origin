# -*- coding: utf-8 -*-
"""Webscraping_script.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fo1wX54L84ldW7sF6EX5y7INaMJCZ286
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# Define the URL to scrape
url = "https://economictimes.indiatimes.com/wealth/insure/life-insurance/latest-life-insurance-claim-settlement-ratio-of-insurance-companies-in-india/articleshow/97366610.cms"

response = requests.get(url)


# Parse the HTML content using BeautifulSoup
soup = BeautifulSoup(response.content, "html")

"""Web Scrapping first table"""

#Table 1
#Scrape all data in table
table_policy=soup.find_all('table')[0]

#Sepatarting the title
table_policy_heading = [title.text for title in (table_policy.find_all('strong'))]

#scraping all rows
row_data_policy = table_policy.find_all('tr')

#Creating a dataframe and assigning the title
df = pd.DataFrame(columns=[data.text for data in row_data_policy[2]])
df.rename_axis(table_policy_heading, axis=1, inplace=True)

#scraping separate cell data
for row in row_data_policy[3:]:
  column_data = row.find_all('td')
  individual_data = [data.text for data in column_data]
  endpoint = len(df)
  df.loc[endpoint] = individual_data

#df

"""Web Scrapping second table"""

#Table 2
#Scraping all data
table_benefit=soup.find_all('table')[1]

#separating the title
table_benefit_heading = [title.text for title in (table_benefit.find_all('strong'))]

#scraping all rows
row_data_benefit = table_benefit.find_all('tr')

#Creating a dataframe and assigning the title
df2 = pd.DataFrame(columns=[data.text for data in row_data_benefit[1]])
df2.rename_axis(table_benefit_heading, axis = 1, inplace = True)

#scraping separate cell data
for row in row_data_benefit[2:]:
  column_data = row.find_all('td')
  individual_data = [data.text for data in column_data]
  endpoint = len(df2)
  df2.loc[endpoint] = individual_data

#df2

"""Saving scrapped data to csv files"""

#saving both dataframes to csv files
df.to_csv('policy.csv')
df2.to_csv('benefit.csv')

#using time so that the api doesnot overwhelm the website
time.sleep(10)